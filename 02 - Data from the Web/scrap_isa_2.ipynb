{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import urllib.parse\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_is_academia_page(page, params={}):\n",
    "    \"\"\"\n",
    "        Return the content of a IS-Academia webpage as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://isa.epfl.ch/imoniteur_ISAP/'\n",
    "    PAGES = {'filter': '!GEDPUBLICREPORTS.filter', 'results': '!GEDPUBLICREPORTS.html'}\n",
    "    BASE_PARAMS = {'ww_i_reportmodel': '133685247', # Registered students by section and semester\n",
    "                   'ww_i_reportModelXsl': '133685270'}\n",
    "    \n",
    "    # Check the validity of the `page` parameter\n",
    "    if not isinstance(page, str) or page not in PAGES:\n",
    "        allowed_pages = ', '.join(PAGES.keys())\n",
    "        raise ValueError('page argument must be: {}'.format(allowed_pages))\n",
    "    \n",
    "    url = urllib.parse.urljoin(BASE_URL, PAGES[page]) # Create the URL\n",
    "    r = requests.get(url, params={**BASE_PARAMS, **params}) # Python 3.5 syntax for merging dictionaries\n",
    "    r.raise_for_status() # Raise an exception if we can't get the page\n",
    "\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dict_from_select(soup, match_name):\n",
    "    # Use a CSS selector to get the <option> tags that are child of\n",
    "    # a <select> tag with the specified `name` attribute\n",
    "    css_selector = 'select[name=\"{}\"] > option'.format(match_name)\n",
    "    option_tags = soup.select(css_selector)\n",
    "    \n",
    "    # Create a dictionary mapping the text to the internal ID used by IS-Acdemia\n",
    "    return {tag.string: tag['value'] for tag in option_tags if tag.string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a strainer to parse only the <select> tags in the page\n",
    "strainer = SoupStrainer('select')\n",
    "page = get_is_academia_page('filter')\n",
    "soup = BeautifulSoup(page, 'html.parser', parse_only=strainer)\n",
    "\n",
    "# Create a dictionary mapping the departments ids\n",
    "departments = construct_dict_from_select(soup, 'ww_x_UNITE_ACAD')\n",
    "cs_department = departments['Informatique']\n",
    "# Create a dictionary mapping the years ids\n",
    "years = construct_dict_from_select(soup, 'ww_x_PERIODE_ACAD')\n",
    "\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_semesters_list(year_id, department_id):\n",
    "    # HTTP parameters\n",
    "    params = {'ww_b_list': 1,\n",
    "              'ww_x_PERIODE_ACAD': year_id,\n",
    "              'ww_x_UNITE_ACAD': department_id}\n",
    "    \n",
    "    page = get_is_academia_page('filter', params)\n",
    "    strainer = SoupStrainer('a') # Only parse the <a> tags\n",
    "    soup = BeautifulSoup(page, 'html.parser', parse_only=strainer)\n",
    "\n",
    "    # Extract the ww_x_GPS id from the Javascript code in the `onclick` attribute of links\n",
    "    def extract_id(tag):\n",
    "        match = re.search('ww_x_GPS=(\\d+)', tag['onclick'])\n",
    "        if match is None: # Return None if there is no positive id associated with this link\n",
    "            return None\n",
    "        else:\n",
    "            return match.group(1)\n",
    "\n",
    "    # Create a dictionary mapping the semesters to their IDs\n",
    "    return {tag.text: extract_id(tag) for tag in soup.find_all('a') if extract_id(tag) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dict mapping all CS semesters from all years to their internal ID used by IS-Academia\n",
    "semester_ids = {}\n",
    "for year_id in years.values():\n",
    "    semester_ids.update(parse_semesters_list(year_id, cs_department))\n",
    "\n",
    "semester_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a dataframe containing the list of students for a given semester\n",
    "# or None if there isn't any table in the page\n",
    "def get_people(semester_id):\n",
    "    page = get_is_academia_page('results', {'ww_x_GPS': semester_id})\n",
    "    \n",
    "    # Parse the HTML table using Pandas\n",
    "    frames = pandas.read_html(page, header=1)\n",
    "    \n",
    "    if frames is None or not frames: # Returns None if we can't get any dataframe\n",
    "        return None\n",
    "    \n",
    "    df = frames[0]\n",
    "    \n",
    "    # Drop the last column which is empty (due to IS-Academia formatting)\n",
    "    return df.drop(df.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not execute the cell below!\n",
    "It gets a lot of pages from IS-Academia. Use the next cell to load the data from a pickle file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#frames = []\n",
    "#for semester, semester_id in semesters_ids.items():\n",
    "#    df = get_people(semester_id)\n",
    "#    \n",
    "#    # Add a column containing the semester (if we can get the dataframe)\n",
    "#    if df is not None:\n",
    "#        df['semester'] = semester\n",
    "#    frames.append(df)\n",
    "#    \n",
    "## Concatenate all semesters from all years\n",
    "#df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('cs_dump.pkl')\n",
    "df.columns = ['title', 'name', 'orientation_ba', 'orientation_ma', 'specialization', 'filiere_opt', 'minor', 'status', 'exchange_type', 'exchange_school', 'sciper', 'semester']\n",
    "#df.set_index(['semester', 'No Sciper'], inplace=True)\n",
    "#df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Take the 'semester' column and split it into 'section', 'years' and 'semester'\n",
    "new_cols = pd.DataFrame(df['semester'], index=df.index)\n",
    "cols = new_cols['semester'].apply(lambda x: pd.Series(x.split(', ')))\n",
    "cols.columns = ['section', 'years', 'semester']\n",
    "data = pd.concat([df.drop('semester', axis=1), cols], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "index1 = ['Bachelor semestre 1' in semester for semester in data.semester]\n",
    "index6 = ['Bachelor semestre 6' in semester for semester in data.semester]\n",
    "\n",
    "list1 = data[index1]\n",
    "list6 = data[index6]\n",
    "\n",
    "started = {}\n",
    "ended = {}\n",
    "total = {}\n",
    "\n",
    "for _, row in list1.iterrows():\n",
    "    sciper = row.sciper\n",
    "    year = row.years.split('-')[0]\n",
    "    if not sciper in started.keys():\n",
    "        started[sciper] = year\n",
    "    elif year < started[sciper]:\n",
    "        started[sciper] = year\n",
    "        \n",
    "for _, row in list6.iterrows():\n",
    "    sciper = row.sciper\n",
    "    year = row.years.split('-')[1]\n",
    "    if not sciper in ended.keys():\n",
    "        ended[sciper] = year\n",
    "    elif year > ended[sciper]:\n",
    "        ended[sciper] = year\n",
    "\n",
    "d = []\n",
    "for key in set(started.keys()).intersection(ended.keys()):\n",
    "    d.append(key)\n",
    "\n",
    "average = {}\n",
    "\n",
    "for sciper in d:\n",
    "    average[sciper] = (int(ended[sciper]) - int(started[sciper]))*12\n",
    "    \n",
    "average\n",
    "df2 = pd.DataFrame(d,columns=['sciper'])\n",
    "m = pd.merge(df2, data[['sciper', 'title']], left_on='sciper', right_on='sciper', how='inner')\n",
    "\n",
    "m.drop_duplicates(inplace=True)\n",
    "group = m.groupby('title')\n",
    "\n",
    "group.count()/len(m)\n",
    "\n",
    "df3 = pd.Series(average)\n",
    "df3\n",
    "\n",
    "m.index = m.sciper\n",
    "m = pd.concat([m, df3], axis=1)\n",
    "m = m.rename({0:'duration'})\n",
    "m.groupby('title').mean()\n",
    "\n",
    "s1 = m[m['title'] == 'Monsieur'][0].tolist()\n",
    "s2 = m[m['title'] == 'Madame'][0].tolist()\n",
    "\n",
    "t_stat, p_val = stats.ttest_ind(s1, s2)\n",
    "p_val\n",
    "\n",
    "m.groupby('title').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_lowest_year(table):\n",
    "    result = {}\n",
    "    for _, row in table.iterrows():\n",
    "        sciper = row.sciper\n",
    "        year = row.years.split('-')[0]\n",
    "        if not sciper in result.keys():\n",
    "            result[sciper] = year\n",
    "        elif year < result[sciper]:\n",
    "            result[sciper] = year\n",
    "    return result\n",
    "\n",
    "def find_highest_year(table):\n",
    "    result = {}\n",
    "    for _, row in table.iterrows():\n",
    "        sciper = row.sciper\n",
    "        year = row.years.split('-')[1]\n",
    "        if not sciper in result.keys():\n",
    "            result[sciper] = year\n",
    "        elif year > result[sciper]:\n",
    "            result[sciper] = year\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ex 2\n",
    "data.semester.unique()\n",
    "index_ma1 = ['Master semestre 1' in semester for semester in data.semester]\n",
    "index_ma2 = ['Master semestre 2' in semester for semester in data.semester]\n",
    "\n",
    "index_mp1 = ['Projet Master printemps' in semester for semester in data.semester]\n",
    "index_mp2 = ['Projet Master automne' in semester for semester in data.semester]\n",
    "\n",
    "ma1 = data[index_ma1]\n",
    "ma2 = data[index_ma2]\n",
    "ma3 = data[index_ma3]\n",
    "\n",
    "mp1 = data[index_mp1]\n",
    "mp2 = data[index_mp2]\n",
    "\n",
    "ma1_ma2 = pd.concat([ma1, ma2], axis=0)\n",
    "mp1_mp2 = pd.concat([mp1, mp2], axis=0)\n",
    "\n",
    "d = []\n",
    "started = find_lowest_year(ma1_ma2)\n",
    "ended = find_lowest_year(ma1_ma2)\n",
    "for key in set(started.keys()).intersection(ended.keys()):\n",
    "    d.append(key)\n",
    "\n",
    "average = {}\n",
    "\n",
    "for sciper in d:\n",
    "    average[sciper] = (int(ended[sciper]) - int(started[sciper]))*12\n",
    "    \n",
    "average\n",
    "\n",
    "ma3[~ma3.minor.isnull()]\n",
    "\n",
    "# Time between ma1 and mp\n",
    "d = []\n",
    "started = find_lowest_year(ma1_ma2)\n",
    "ended = find_highest_year(mp1_mp2)\n",
    "for key in set(started.keys()).intersection(ended.keys()):\n",
    "    d.append(key)\n",
    "\n",
    "average = {}\n",
    "\n",
    "for sciper in d:\n",
    "    average[sciper] = (int(ended[sciper]) - int(started[sciper]))*12\n",
    "    \n",
    "average\n",
    "\n",
    "df2 = pd.DataFrame(d,columns=['sciper'])\n",
    "m = pd.merge(df2, data[['sciper', 'title', 'specialization']], left_on='sciper', right_on='sciper', how='inner')\n",
    "m.drop_duplicates(inplace=True)\n",
    "\n",
    "m.drop_duplicates('sciper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.semester.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
