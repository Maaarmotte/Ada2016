{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import urllib.parse\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_is_academia_page(page, params={}):\n",
    "    \"\"\"\n",
    "        Return the content of a IS-Academia webpage as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = 'https://isa.epfl.ch/imoniteur_ISAP/'\n",
    "    PAGES = {'filter': '!GEDPUBLICREPORTS.filter', 'results': '!GEDPUBLICREPORTS.html'}\n",
    "    BASE_PARAMS = {'ww_i_reportmodel': '133685247', # Registered students by section and semester\n",
    "                   'ww_i_reportModelXsl': '133685270'}\n",
    "    \n",
    "    # Check the validity of the `page` parameter\n",
    "    if not isinstance(page, str) or page not in PAGES:\n",
    "        allowed_pages = ', '.join(PAGES.keys())\n",
    "        raise ValueError('page argument must be: {}'.format(allowed_pages))\n",
    "    \n",
    "    url = urllib.parse.urljoin(BASE_URL, PAGES[page]) # Create the URL\n",
    "    r = requests.get(url, params={**BASE_PARAMS, **params}) # Python 3.5 syntax for merging dictionaries\n",
    "    r.raise_for_status() # Raise an exception if we can't get the page\n",
    "\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_dict_from_select(soup, match_name):\n",
    "    # Use a CSS selector to get the <option> tags that are child of\n",
    "    # a <select> tag with the specified `name` attribute\n",
    "    css_selector = 'select[name=\"{}\"] > option'.format(match_name)\n",
    "    option_tags = soup.select(css_selector)\n",
    "    \n",
    "    # Create a dictionary mapping the text to the internal ID used by IS-Acdemia\n",
    "    return {tag.string: tag['value'] for tag in option_tags if tag.string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a strainer to parse only the <select> tags in the page\n",
    "strainer = SoupStrainer('select')\n",
    "page = get_is_academia_page('filter')\n",
    "soup = BeautifulSoup(page, 'html.parser', parse_only=strainer)\n",
    "\n",
    "# Create a dictionary mapping the departments ids\n",
    "departments = construct_dict_from_select(soup, 'ww_x_UNITE_ACAD')\n",
    "cs_department = departments['Informatique']\n",
    "# Create a dictionary mapping the years ids\n",
    "years = construct_dict_from_select(soup, 'ww_x_PERIODE_ACAD')\n",
    "\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_semesters_list(year_id, department_id):\n",
    "    # HTTP parameters\n",
    "    params = {'ww_b_list': 1,\n",
    "              'ww_x_PERIODE_ACAD': year_id,\n",
    "              'ww_x_UNITE_ACAD': department_id}\n",
    "    \n",
    "    page = get_is_academia_page('filter', params)\n",
    "    strainer = SoupStrainer('a') # Only parse the <a> tags\n",
    "    soup = BeautifulSoup(page, 'html.parser', parse_only=strainer)\n",
    "\n",
    "    # Extract the ww_x_GPS id from the Javascript code in the `onclick` attribute of links\n",
    "    def extract_id(tag):\n",
    "        match = re.search('ww_x_GPS=(\\d+)', tag['onclick'])\n",
    "        if match is None: # Return None if there is no positive id associated with this link\n",
    "            return None\n",
    "        else:\n",
    "            return match.group(1)\n",
    "\n",
    "    # Create a dictionary mapping the semesters to their IDs\n",
    "    return {tag.text: extract_id(tag) for tag in soup.find_all('a') if extract_id(tag) is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dict mapping all CS semesters from all years to their internal ID used by IS-Academia\n",
    "semester_ids = {}\n",
    "for year_id in years.values():\n",
    "    semester_ids.update(parse_semesters_list(year_id, cs_department))\n",
    "\n",
    "semester_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a dataframe containing the list of students for a given semester\n",
    "# or None if there isn't any table in the page\n",
    "def get_people(semester_id):\n",
    "    page = get_is_academia_page('results', {'ww_x_GPS': semester_id})\n",
    "    \n",
    "    # Parse the HTML table using Pandas\n",
    "    frames = pandas.read_html(page, header=1)\n",
    "    \n",
    "    if frames is None or not frames: # Returns None if we can't get any dataframe\n",
    "        return None\n",
    "    \n",
    "    df = frames[0]\n",
    "    \n",
    "    # Drop the last column which is empty (due to IS-Academia formatting)\n",
    "    return df.drop(df.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not execute the cell below!\n",
    "It gets a lot of pages from IS-Academia. Use the next cell to load the data from a pickle file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "for semester, semester_id in semesters_ids.items():\n",
    "    df = get_people(semester_id)\n",
    "    \n",
    "    # Add a column containing the semester (if we can get the dataframe)\n",
    "    if df is not None:\n",
    "        df['semester'] = semester\n",
    "    frames.append(df)\n",
    "    \n",
    "# Concatenate all semesters from all years\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data from a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('cs_dump.pkl')\n",
    "#df.set_index(['semester', 'No Sciper'], inplace=True)\n",
    "#df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = df[df['Nom Pr√©nom'].str.contains('^Habegger\\sL')]\n",
    "tmp.semester.apply(lambda s: s.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
