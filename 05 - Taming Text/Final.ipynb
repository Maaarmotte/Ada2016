{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Wordclouds\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = pd.read_csv(\"hillary-clinton-emails/Emails.csv\")\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we inspect what data we are given. For the wordcloud it would be wise to use only the \"ExtractedBodyText\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing emails without body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails.size #Size of the dataset before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emails = emails[pd.notnull(emails['ExtractedBodyText'])]\n",
    "emails.size #Size of the dataset after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Generating the wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Converting the ExtractedBodyText into strings we can use for the wordcloud. Note that we tell WordCloud to not remove any WordCloud for this first graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = emails.ExtractedBodyText.to_string();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords=['']).generate(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the wordcloud we will go through the process of removing stopwords. Then we will make sure to have only alphabetical characters and use stemming to convert words into their original base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text) # Tokenizing the text\n",
    "    tokens = [i for i in tokens if i.isalpha()] #Keeping only strings with alphabetic characters\n",
    "    tokens = [i for i in tokens if i.lower() not in sw] #Removing stopwords\n",
    "    stems = stem_tokens(tokens, stemmer) #Stemming the words\n",
    "    return stems  #returns a list\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "improvedText = str(tokenize(text)).replace(\"'\", \"\") #Convert from list to string object without single quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc = WordCloud().generate(improvedText)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the two wordclouds, we see that they are largely similar if we don't take stopwords into account. In the first cloud we have 'nFriday' and 'nSunday' as backslashes are converted to 'n'.  In the second cloud some words are shortened to their stems such as 'Novemb' and 'relea'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the models.ldamodel module from the gensim library, run topic modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which returns topics that you consider to be meaningful at first sight.\n",
    "\n",
    "Start by important gensim libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim import corpora\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data, keep only the ExtractedBodyText cells and remove null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails = pd.DataFrame(pd.read_csv('hillary-clinton-emails/Emails.csv')['ExtractedBodyText'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the same functions defined in the first exercice to remove the stopwords, remove numbers and stem tokens. Note that we also lower all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emails['ExtractedBodyText'] = emails.ExtractedBodyText.apply(lambda x: [word.lower() for word in tokenize(x) if word not in sw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the tokenized emails into a list of lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = emails.ExtractedBodyText.tolist()\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the list of lists of tokens into a corpus (a list of Bag of Words), and then train the Latent Dirichlet Allocation model on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make a function to print the topics in a concise way, for a better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_topics(topics):\n",
    "    for topic in topics:\n",
    "        print(\"Topic {}:\\t\".format(topic[0]), end=\"\")\n",
    "        for tup in topic[1]:\n",
    "            print(\"{}, \".format(tup[0], tup[1]), end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find 5 topics in the corpus using the Latent Dirichlet Allocation model. Note that running the algorithm can take a bit of time, due to the number of passes. To avoid having to train the model multiple times, the topics are stored in two files. It is also better to discuss to topics, because they are not exactly the same between multiple runs. Increasing the number of passes (default is 1) is good because it increases accuracy of topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pickle.load(open('5_topics.p', 'rb'))\n",
    "except:\n",
    "    model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=50)\n",
    "    data = model.show_topics(num_topics=5, num_words=10, formatted=False)\n",
    "    pickle.dump(data, open('5_topics.p', 'wb'))\n",
    "\n",
    "format_topics(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the topics are generic. They are about Obama, diplomacy and politics in general. For example, topic 0 is about elections ('elect', 'vote') and american politics ('democrat', 'republican', 'parti', 'senat'). We can compare these topics with the one generated by the LDA model when looking for 50 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pickle.load(open('50_topics.p', 'rb'))\n",
    "except:\n",
    "    model = LdaModel(corpus, num_topics=50, id2word=dictionary, passes=50)\n",
    "    data = model.show_topics(num_topics=50, num_words=10, formatted=False)\n",
    "    pickle.dump(data, open('50_topics.p', 'wb'))\n",
    "    \n",
    "format_topics(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, they are more diverse. Instead of having one topic for diplomacy, we have one topic about pakistan (27), one about china (24), one about israel and palestinia (7), etc. There are also topics about recent events: North Korea testing nuclear weapons (27), attack in Benghazi (26), etc. We could say that 50 topics is too much because some topics seem to be not related to anything (2, 49, ...), but it is not necessarily true since some topics (Russia for example) do not appear at each run of the LDA model. Note that his can also be because the word 'Russia' is not important enough to be in the top ten words of any topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pickle.load(open('25_topics.p', 'rb'))\n",
    "except:\n",
    "    model = LdaModel(corpus, num_topics=25, id2word=dictionary, passes=50)\n",
    "    data = model.show_topics(num_topics=25, num_words=10, formatted=False)\n",
    "    pickle.dump(data, open('25_topics.p', 'wb'))\n",
    "    \n",
    "format_topics(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When limited to 25, the topics are more centered around US politics ('democrat', 'republican', 'vote', 'elect', 'senat', 'bill', 'parti', 'presid', ...), meetings ('call', 'talk', 'tomorrow', 'work', 'offic', 'meet', 'room', ...) and middle east ('iran', 'israel', 'border', 'jewish', 'palestinian', 'ahmadinejad', 'peac', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best number of topics to look for actually depends on how deep a person wants to go. If set to 5, then only generic topics will emerge with not much information, and it probably won't be enough for most cases. With 50, more detailed topics appear, specialized for different countries the US are working with. But with these also come less meaningful topics. A good compromise would be to find a value between 5 and 25. For example, with 25, the topics are not too generic, and there aren't too much missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Bonus - Communication graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
